{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "sys.path.append('/opt/conda/envs/python37/lib/python3.7/site-packages')\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from shapely.geometry import Point, Polygon\n",
    "from geopandas import GeoSeries,GeoDataFrame\n",
    "import folium.plugins as plugins\n",
    "import re\n",
    "import seaborn as sns\n",
    "import mapclassify\n",
    "import PIL\n",
    "import io\n",
    "import os\n",
    "import plot_map\n",
    "from shapely.ops import nearest_points\n",
    "import math\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入地图的信息\n",
    "grid = gpd.read_file('/root/Data/泉州整合空间数据【0426】/泉州整合空间数据【0426】/yuwang/qzyw.shp')\n",
    "grid = grid.to_crs(\"EPSG:4326\")\n",
    "grid['center'] = grid['geometry'].representative_point()\n",
    "grid['lng'] = grid['center'].x\n",
    "grid['lat'] = grid['center'].y\n",
    "grid.rename(columns={'FID_1':'FID'},inplace=True)\n",
    "IDID_list = grid['IDID'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 行政区划\n",
    "\n",
    "attr = pd.read_excel('/root/Data/泉州整合空间数据【0426】/泉州整合空间数据【0426】/QZ_指标finally【0426】.xlsx')\n",
    "district = set(attr['行政区'].to_list())\n",
    "print(district)\n",
    "\n",
    "quhua = gpd.read_file('/root/Data/quhua/ok_geo.csv')\n",
    "qz_quhua = quhua.iloc[1171:1184,:]\n",
    "\n",
    "def geo(polygon):\n",
    "    t = re.split('[,;]',polygon)\n",
    "    t = [i.split(' ') for i in t]\n",
    "    t = [[float(i[0]),float(i[1])] for i in t]\n",
    "    return Polygon(t)\n",
    "\n",
    "qz_quhua['geometry'] = qz_quhua.apply(lambda x: geo(x['polygon']), axis=1)\n",
    "\n",
    "dis = list(district)[2:]\n",
    "dis = pd.DataFrame(dis,columns=['name'])\n",
    "dis_shp = qz_quhua.merge(dis,how='right')\n",
    "dis_shp\n",
    "\n",
    "tmp_c = quhua.iloc[1171]['geo'].split(' ')\n",
    "city_map = folium.Map(location=[float(tmp_c[1]),float(tmp_c[0])],zoom_start=10)\n",
    "folium.GeoJson(\n",
    "    qz_quhua[1:].iloc[1:].to_json(),\n",
    ").add_to(city_map)\n",
    "city_map\n",
    "\n",
    "qu_shp = gpd.read_file('/root/Projects/dengwanting/grid_quhua/quhua.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_half_hour_all = pd.read_csv('/root/zp/processedData/per_half_hour/slice_data/qz_lbs_20200218_slice.csv')\n",
    "\n",
    "data_per_half_hour_all_ = gpd.GeoDataFrame(\n",
    "    data_per_half_hour_all.loc[:, [c for c in data_per_half_hour_all.columns if c != \"geometry\"]],\n",
    "    geometry=gpd.GeoSeries.from_wkt(data_per_half_hour_all[\"geometry\"]),\n",
    "    crs=\"epsg:4326\",\n",
    ")\n",
    "\n",
    "data_per_half_hour_all_gpd = gpd.GeoDataFrame(data_per_half_hour_all_,geometry='geometry')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "data_per_half_hour_all_gpd.plot(ax=ax,cmap='Reds',scheme='userdefined',classification_kwds={'bins':[0,10,50,100,200,550]},column='H0',legend=True)\n",
    "qu_shp.plot(ax=ax,edgecolor = (0,0,0,1),facecolor = (0,0,0,0),linewidths = 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected = data_per_half_hour_all_gpd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 地块编号矩阵\n",
    "min_x,max_x,min_y,max_y = 118.4159,118.9855,24.6397,25.2051\n",
    "interval_x,interval_y = 0.00494,0.00451\n",
    "# row,col = math.ceil((max_y-min_y)/interval_y),math.ceil((max_x-min_x)/interval_x)\n",
    "# row,col\n",
    "# index_mat = np.zeros([row,col])\n",
    "\n",
    "# for i in range(len(data_selected)):\n",
    "#     x = math.floor((data_selected.iloc[i]['lng']-min_x)/interval_x + 0.5)\n",
    "#     y = math.floor((data_selected.iloc[i]['lat']-min_y)/interval_y + 0.5)\n",
    "#     index_mat[y][x] = data_selected.iloc[i]['IDID']\n",
    "index_mat = np.load('/root/zp/flow_data/index_mat.npy')\n",
    "row,col = index_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 人流密度矩阵\n",
    "density_mat = dict()\n",
    "for i in range(48):\n",
    "    key = 'density_mat_H' + str(i)\n",
    "    # value = np.zeros([row,col])\n",
    "    # time = 'H' + str(i)\n",
    "    # for j in range(len(data_selected)):\n",
    "    #     x = math.floor((data_selected.iloc[j]['lng']-min_x)/interval_x + 0.5)\n",
    "    #     y = math.floor((data_selected.iloc[j]['lat']-min_y)/interval_y + 0.5)\n",
    "    #     value[y][x] = data_selected.iloc[j][time]\n",
    "    # np.save('/root/zp/flow_data/20190219/density_mat/' + key + '.npy', value)\n",
    "    # density_mat[key] = value\n",
    "    density_mat[key] = np.load('/root/zp/flow_data/20190219/density_mat/' + key + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 计算源与流\n",
    "for i in range(48):\n",
    "    key = 'density_mat_H' + str(i)\n",
    "    \n",
    "    test_density_mat = density_mat[key]\n",
    "    test_density_mat_pad = np.pad(test_density_mat, 1)  # 矩阵外圈补0\n",
    "\n",
    "    test_divergence_mat = np.zeros([row, col])\n",
    "    test_gradient_x_mat = np.zeros([row, col])\n",
    "    test_gradient_y_mat = np.zeros([row, col])\n",
    "\n",
    "    for j in range(1, test_density_mat_pad.shape[0]-1):\n",
    "        for k in range(1, test_density_mat_pad.shape[1]-1):\n",
    "            # -4 1\n",
    "            test_divergence_mat[j-1, k-1] = (test_density_mat_pad[j-1, k] + test_density_mat_pad[j+1, k] + \\\n",
    "                                             test_density_mat_pad[j, k-1] + test_density_mat_pad[j, k+1]) - (4 * test_density_mat_pad[j, k])\n",
    "            # -8 1\n",
    "            # test_divergence_mat[j-1, k-1] = (test_density_mat_pad[j-1, k] + test_density_mat_pad[j+1, k] + \\\n",
    "            #                         test_density_mat_pad[j, k-1] + test_density_mat_pad[j, k+1] + test_density_mat_pad[j-1,k-1] + \\\n",
    "            #                         test_density_mat_pad[j-1,k+1] + test_density_mat_pad[j+1, k-1] + test_density_mat_pad[j+1, k+1]) \\\n",
    "            #                         - (4 * test_density_mat_pad[j, k])\n",
    "            \n",
    "            # test_gradient_x_mat[j-1, k-1] = test_density_mat_pad[j, k+1] - test_density_mat_pad[j, k-1]\n",
    "            # test_gradient_y_mat[j-1, k-1] = test_density_mat_pad[j-1, k] - test_density_mat_pad[j+1, k]\n",
    "            \n",
    "    np.save('/root/zp/flow_data/20190219/divergence_mat/' + 'divergence_mat_H' + str(i) + '.npy', test_divergence_mat)\n",
    "    # np.save('/root/zp/flow_data/20190219/gradient_x_mat/' + 'gradient_x_mat_H' + str(i) + '.npy', test_gradient_x_mat)\n",
    "    # np.save('/root/zp/flow_data/20190219/gradient_y_mat/' + 'gradient_y_mat_H' + str(i) + '.npy', test_gradient_y_mat)\n",
    "    \n",
    "    # test_divergence_mat_adjust = np.zeros([row, col])\n",
    "    # for m in range(row):\n",
    "    #     for n in range(col):\n",
    "    #         if test_divergence_mat[m, n] > 0:\n",
    "    #             test_divergence_mat_adjust[m, n] = 1\n",
    "    #         elif test_divergence_mat[m, n] < 0:\n",
    "    #             test_divergence_mat_adjust[m, n] = -1\n",
    "                \n",
    "    # key2 = 'divergence_H' + str(i)\n",
    "    # data_per_half_hour_all_gpd[key2] = 0\n",
    "\n",
    "    # for u in range(row):\n",
    "    #     for v in range(col):\n",
    "    #         if index_mat[u, v] != 0:\n",
    "    #             block_id = index_mat[u, v]\n",
    "    #             index = data_per_half_hour_all_gpd[data_per_half_hour_all_gpd.IDID == block_id].index.values[0]\n",
    "    #             # data_per_half_hour_all_gpd.loc[index, key2] = test_divergence_mat_adjust[u, v]\n",
    "    #             data_per_half_hour_all_gpd.loc[index, key2] = test_divergence_mat[u, v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_per_half_hour_all_gpd.copy()\n",
    "\n",
    "b = ['IDID','lng','lat','geometry']\n",
    "for i in range(48):\n",
    "    a.rename(columns={'divergence_H' + str(i) : 'div_H' + str(i)}, inplace=True)\n",
    "    b.append('div_H' + str(i))\n",
    "a = a[b]\n",
    "a.to_file('/root/zp/flow_data/20190219/divergence_new.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
